{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import xgboost as xgb\n",
    "import scikitplot as skplt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "from joblib import dump, load\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import genesis\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "datasets = {'test' : './test/testing.csv', 'train' : './train/training.csv'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jfear\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jfear\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jfear\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jfear\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     C:\\Users\\jfear\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\genesis.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('genesis')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PlaceInCsvs():\n",
    "    directories = ['./test/', './train/']\n",
    "    subdirectories = ['neg', 'pos']\n",
    "\n",
    "    for directory in directories:\n",
    "        ratings = []\n",
    "        reviews = []\n",
    "        sentiments = []\n",
    "        for subdirectory in subdirectories:\n",
    "            fullDirectory = f'{directory}{subdirectory}'\n",
    "            for file in os.listdir(fullDirectory):\n",
    "                rating = int((file.split('_', 1)[1])[0])\n",
    "                if 'neg' in fullDirectory and rating >= 5:\n",
    "                    rating = pd.NA\n",
    "                elif 'pos' in fullDirectory and rating <= 4:\n",
    "                    rating = pd.NA\n",
    "                \n",
    "                \n",
    "                if 'pos' in fullDirectory:\n",
    "                    sentiments.append(1)\n",
    "                else: \n",
    "                    sentiments.append(0)\n",
    "                    \n",
    "                reviewFile = open(f'{fullDirectory}/{file}', 'r', encoding='utf-8')\n",
    "                review = reviewFile.read()\n",
    "                ratings.append(rating)\n",
    "                reviews.append(review)\n",
    "        df = pd.DataFrame()\n",
    "        df['rating'] = ratings\n",
    "        df['review'] = reviews\n",
    "        df['sentiment'] = sentiments\n",
    "        df['rating'] = df['rating'].astype({'rating':'Int64'}, copy=False)\n",
    "        fileNames = {f'{directories[0]}' : 'testing', f'{directories[1]}' : 'training'}\n",
    "        df.to_csv(f'{directory}{fileNames[directory]}.csv')\n",
    "        \n",
    "def RemoveHTML(dataframe, fileName):\n",
    "    regex = re.compile('<.*?>')\n",
    "    for review in dataframe['review']:\n",
    "        cleanReview = re.sub(regex, '', review)\n",
    "        dataframe.loc[dataframe.review == review, 'review'] = cleanReview \n",
    "    dataframe.to_csv(fileName, index=False)\n",
    "    \n",
    "def AddTitleIds(dataframe):\n",
    "    directories = ['./test/', './train/']\n",
    "    subdirectories = ['neg', 'pos']\n",
    "    regex = re.compile('<.*?>')\n",
    "    \n",
    "\n",
    "    for directory in directories:\n",
    "        for subdirectory in subdirectories:\n",
    "            urlFile = open(f'{directory}/urls_{subdirectory}.txt', 'r', encoding='utf-8')\n",
    "            fullDirectory = f'{directory}{subdirectory}'\n",
    "            for file in os.listdir(fullDirectory):\n",
    "                reviewFile = open(f'{fullDirectory}/{file}', 'r', encoding='utf-8')\n",
    "                reviewText = re.sub(regex, '', reviewFile.read())\n",
    "                dataframe.loc[dataframe.reviews == reviewText, 'reviews'].iloc[0]['reviews']\n",
    "                \n",
    "def AddPositiveNegativeLabel(filePath):\n",
    "    dataframe = pd.read_csv(filePath, index_col='index')\n",
    "    dataframe['sentiment'] = 0\n",
    "    for row in dataframe.iterrows():\n",
    "        if dataframe.at[row[0], 'ratings'] <= 4:\n",
    "            dataframe.at[row[0], 'sentiment'] = 0\n",
    "        else:\n",
    "            dataframe.at[row[0], 'sentiment'] = 1\n",
    "    dataframe.to_csv(filePath)\n",
    "    \n",
    "def CreateNGrams(data):\n",
    "    dataframe = pd.read_csv('./train/training_new.csv')\n",
    "    vectorizer = CountVectorizer(lowercase=True, ngram_range=(1,1), min_df=5, max_df=0.5, binary=False, strip_accents='unicode')\n",
    "    #transformer = TfidfTransformer(use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "    \n",
    "    vectorizer.fit(dataframe['review'].values.astype('U'))\n",
    "    vector = vectorizer.transform(data)\n",
    "    vector.toarray()\n",
    "    #transformer.fit(vectorizer.transform(dataframe['review'].values.astype('U')))\n",
    "    #transformer.transform(vectorizer.transform(data))\n",
    "    return vector\n",
    "    \n",
    "    \n",
    "def CleanTextFurther():\n",
    "    csvs = ['./test/testing', './train/training']\n",
    "    stopWords = stopwords.words('english')   \n",
    "    genesisIc = wordnet.ic(genesis, False, 0.0)\n",
    "    #stopWords.remove('very')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    for path in csvs:\n",
    "        dataframe = pd.read_csv(f'{path}.csv') \n",
    "        dataframe.drop(dataframe.loc[dataframe['rating'].isna()].index, inplace=True)\n",
    "        reviews = list(dataframe['review'].values)\n",
    "        for review in reviews:\n",
    "            reviewIndex = reviews.index(review)\n",
    "            review = contractions.fix(review)\n",
    "            review = re.sub(r'[^\\w\\s]', ' ', review)\n",
    "            words = word_tokenize(review)\n",
    "            words = [word for word in words if word not in set(stopWords)]\n",
    "            wordTags = nltk.pos_tag(words)\n",
    "            \n",
    "            # Remove Numbers and Proper Nouns\n",
    "            for word, tag in wordTags:\n",
    "                if word.isnumeric():\n",
    "                    review = review.replace(f' {word} ', ' ')\n",
    "                if tag == 'NNP':\n",
    "                    review = review.replace(f' {word} ', ' ')\n",
    "                # if tag != 'NN' and tag != 'RP' and tag != 'JJR' and tag != 'JJS' and tag != 'RB' and tag != 'RBR' and tag != 'RBS':\n",
    "                #     review = review.replace(f' {word} ', ' ')\n",
    "            words = word_tokenize(review.lower())\n",
    "            review = ''\n",
    "            for word in words:\n",
    "                #word = lemmatizer.lemmatize(word)\n",
    "                # for lemma in wordnet.synsets(word, pos=['a']):\n",
    "                #     if lemma.pos() != 'a':\n",
    "                #         continue\n",
    "                #     if lemma.res_similarity(wordnet.synset('good.a.01'), genesisIc) >= 6.0:\n",
    "                #         word = lemmatizer.lemmatize('good')\n",
    "                #     if lemma.res_similarity(wordnet.synset('bad.a.01'), genesisIc) >= 6.0:\n",
    "                #         word = lemmatizer.lemmatize('bad')\n",
    "                word = stemmer.stem(word)\n",
    "                review = review + f'{word} '\n",
    "            review = ' '.join([word for word in words if word not in set(stopWords)])\n",
    "            # Remove Spaces\n",
    "            while review.count('  ') >= 1:\n",
    "                review = review.replace('  ', ' ')\n",
    "            reviews[reviewIndex] = review\n",
    "        dataframe['review'] = reviews\n",
    "        dataframe.drop(dataframe.loc[dataframe['review'].isna()].index, inplace=True)\n",
    "        dataframe.to_csv(f'{path}_new.csv', index=False)\n",
    "                 \n",
    "    \n",
    "def TrainModel(model):\n",
    "    dataframe = pd.read_csv('./train/training_new.csv')\n",
    "    \n",
    "    bigrams = CreateNGrams(dataframe['review'].values.astype('U'))\n",
    "    \n",
    "    xTrain, xValidate, yTrain, yValidate = train_test_split(bigrams, dataframe['sentiment'].values, train_size=0.75, stratify=dataframe['sentiment'])\n",
    "\n",
    "    # Testing both types of models\n",
    "\n",
    "    if model == 'xgb':\n",
    "        classifier = xgb.XGBClassifier(n_estimators=10000, max_depth=4, learning_rate=0.2, tree_method='gpu_hist', early_stopping_rounds=50)\n",
    "        classifier.fit(xTrain, yTrain, eval_set=[(xTrain, yTrain), (xValidate, yValidate)], verbose=100)\n",
    "        classifier.save_model('xgb_model.json')\n",
    "    elif model == 'mnb':\n",
    "        classifier = MultinomialNB(fit_prior=False, alpha=0.25)\n",
    "        classifier.fit(bigrams, dataframe['sentiment'].values)\n",
    "        dump(classifier, 'mnb_model.sav')\n",
    "    \n",
    "    \n",
    "def TestModel(model):\n",
    "    #CreateNGrams('./test/testing.csv', 'test_bigrams', 'test_freq')\n",
    "    testDataframe = pd.read_csv('./test/testing_new.csv')\n",
    "    \n",
    "    bigrams = CreateNGrams(testDataframe['review'].values.astype('U'))\n",
    "    \n",
    "    #matrix = xgb.DMatrix(bigrams, testDataframe['sentiment'].values)\n",
    "    \n",
    "    prediction = None\n",
    "    \n",
    "    if model == 'xgb':\n",
    "        classifier = xgb.XGBClassifier(n_estimators=10000, max_depth=4, learning_rate=0.1, tree_method='gpu_hist', early_stopping_rounds=50)    \n",
    "        classifier.load_model('xgb_model.json')\n",
    "        prediction = classifier.predict(bigrams)\n",
    "    elif model == 'mnb':\n",
    "        classifier = load('mnb_model.sav')\n",
    "        prediction = classifier.predict(bigrams)\n",
    "    \n",
    "    \n",
    "    #display(prediction)\n",
    "    #display(testDataframe['sentiment'].values)\n",
    "    \n",
    "    print('F1-Score: ')\n",
    "    print(sklearn.metrics.f1_score(testDataframe['sentiment'].values, prediction))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.65714\tvalidation_1-logloss:0.65771\n",
      "[100]\tvalidation_0-logloss:0.32825\tvalidation_1-logloss:0.40233\n",
      "[200]\tvalidation_0-logloss:0.25438\tvalidation_1-logloss:0.36883\n",
      "[300]\tvalidation_0-logloss:0.21233\tvalidation_1-logloss:0.35515\n",
      "[400]\tvalidation_0-logloss:0.18242\tvalidation_1-logloss:0.34801\n",
      "[500]\tvalidation_0-logloss:0.15946\tvalidation_1-logloss:0.34179\n",
      "[600]\tvalidation_0-logloss:0.14078\tvalidation_1-logloss:0.33979\n",
      "[700]\tvalidation_0-logloss:0.12666\tvalidation_1-logloss:0.33890\n",
      "[800]\tvalidation_0-logloss:0.11500\tvalidation_1-logloss:0.33763\n",
      "[832]\tvalidation_0-logloss:0.11117\tvalidation_1-logloss:0.33792\n",
      "F1-Score: \n",
      "0.8081110506260206\n"
     ]
    }
   ],
   "source": [
    "#CleanTextFurther()\n",
    "TrainModel('xgb')\n",
    "TestModel('xgb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43805874bde62c88d4bae267f8686469aeea3aace9b6f9640b0c8df5d84c6d47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
